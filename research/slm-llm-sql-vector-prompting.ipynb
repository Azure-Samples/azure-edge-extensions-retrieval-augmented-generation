{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLM/LLMs on edge with Prompting across CSV/SQL and Vector Store Experiments\n",
    "\n",
    "This notebook is focused at experiments with Phi-2, LLama2 and Llamma2 chat model and how prompting can get different results.\n",
    "\n",
    "Pre-requisites: \n",
    "- Download the quantized models and update the references below to point to the files for Phi2, Llama2 and Llama2 Chat - see [Readme](../src/rag-on-edge-LLM-32core/README.md)\n",
    "- Update the pointers to the files below in the variables `modelFilePhi`, `modelFileLlama2`, `modelFileLlam2Chat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "%pip install python-dotenv\n",
    "%pip install langchain==0.1.11 \n",
    "%pip install llama_cpp_python==0.2.43\n",
    "%pip install langchain-community==0.0.27\n",
    "%pip install langchain-core==0.1.30\n",
    "%pip install langsmith==0.0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.llms import LlamaCpp\n",
    "import time\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "N_THREADS = int(os.getenv('N_THREADS', os.cpu_count()))\n",
    "modelFilePhi = \"../src/rag-on-edge-LLM-32core/modules/LLMModule/models/phi-2.Q5_K_M.gguf\"\n",
    "modelFileLlama2 = \"../src/rag-on-edge-LLM-32core/modules/LLMModule/models/llama-2-7b.Q4_K_M.gguf\"\n",
    "modelFileLlam2Chat = \"../src/rag-on-edge-LLM-32core/modules/LLMModule/models/llama-2-7b-chat.Q4_K_M.gguf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../src/rag-on-edge-LLM-32core/modules/LLMModule/models/llama-2-7b.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.10 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Llama 2 model\n",
    "# default temperature via Llama.cpp is 0.8\n",
    "llmmodelLlama2 = LlamaCpp(model_path=modelFileLlama2, verbose=True, n_threads=N_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Llama 2 chat model\n",
    "llmmodelLlamaChat2 = LlamaCpp(model_path=modelFileLlam2Chat, temperature=0.8, verbose=False, n_threads=N_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init with temperature settings\n",
    "llmmodelLlamaChat01Temp = LlamaCpp(model_path=modelFileLlam2Chat, temperature=0.1, verbose=False, n_threads=N_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model \n",
    "llmmodelPhi = LlamaCpp(model_path=modelFilePhi, verbose=False, n_threads=N_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 325 tensors from ../src/rag-on-edge-LLM-32core/modules/LLMModule/models/phi-2.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi2\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi2\n",
      "llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 10240\n",
      "llama_model_loader: - kv   5:                           phi2.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  195 tensors\n",
      "llama_model_loader: - type q5_K:   81 tensors\n",
      "llama_model_loader: - type q6_K:   49 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 910/51200 vs 944/51200 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 51200\n",
      "llm_load_print_meta: n_merges         = 50000\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2560\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 32\n",
      "llm_load_print_meta: n_embd_head_k    = 80\n",
      "llm_load_print_meta: n_embd_head_v    = 80\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2560\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2560\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 10240\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 2.78 B\n",
      "llm_load_print_meta: model size       = 1.93 GiB (5.96 BPW) \n",
      "llm_load_print_meta: general.name     = Phi2\n",
      "llm_load_print_meta: BOS token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.12 MiB\n",
      "llm_load_tensors:        CPU buffer size =  1974.94 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.10 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.64 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '50256', 'tokenizer.ggml.eos_token_id': '50256', 'tokenizer.ggml.bos_token_id': '50256', 'general.architecture': 'phi2', 'general.name': 'Phi2', 'phi2.context_length': '2048', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_bos_token': 'false', 'phi2.embedding_length': '2560', 'phi2.attention.head_count': '32', 'phi2.attention.head_count_kv': '32', 'phi2.feed_forward_length': '10240', 'phi2.attention.layer_norm_epsilon': '0.000010', 'phi2.block_count': '32', 'phi2.rope.dimension_count': '32', 'general.file_type': '17'}\n"
     ]
    }
   ],
   "source": [
    "llmmodelPhi01Temp = LlamaCpp(model_path=modelFilePhi, temperature=0.1, verbose=True, n_threads=N_THREADS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to use throughout the notebook\n",
    "\n",
    "def llm_inference(model, promptData):\n",
    "    \n",
    "    llm_response = model.invoke(promptData)\n",
    "    llm_response_str=str(llm_response)\n",
    "    \n",
    "    return llm_response_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The original question is given below.\n",
      "This question has been translated into a SQL query. Both the SQL query and the response are given below.\n",
      "Given the SQL response, the question has also been translated into a vector store query.\n",
      "The vector store query and response is given below.\n",
      "Given SQL query, SQL response, transformed vector store query, and vector store response, please synthesize a response to the original question.\n",
      "\n",
      "Original question: What is the capital of France?\n",
      "SQL query: SELECT capital FROM countries WHERE country = 'France'\n",
      "SQL response: Paris\n",
      "Transformed vector store query: Explain why Paris is the capital of France.\n",
      "Vector store response: Paris is called the city of light. It is the capital of France.\n",
      "Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the prompt variable template\n",
    "# Experimenting with the pattern and idea from LlamaIndex SQL Vector Query Engine. Using the same prompt template for experimentation only.\n",
    "# See https://github.com/run-llama/llama_index/blob/f35ea605adaecf778ad3ffe43354ab74452b8195/llama-index-core/llama_index/core/query_engine/sql_vector_query_engine.py\n",
    "\n",
    "DEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT_TMPL = \"\"\"\n",
    "The original question is given below.\n",
    "This question has been translated into a SQL query. \\\n",
    "Both the SQL query and the response are given below.\n",
    "Given the SQL response, the question has also been translated into a vector store query.\n",
    "The vector store query and response is given below.\n",
    "Given SQL query, SQL response, transformed vector store query, and vector store \\\n",
    "response, please synthesize a response to the original question.\n",
    "\n",
    "Original question: {query_str}\n",
    "SQL query: {sql_query_str}\n",
    "SQL response: {sql_response_str}\n",
    "Transformed vector store query: {query_engine_query_str}\n",
    "Vector store response: {query_engine_response_str}\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "# replace the variables with text\n",
    "replaced = DEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT_TMPL.format(query_str=\"What is the capital of France?\",\n",
    "                                                        sql_query_str=\"SELECT capital FROM countries WHERE country = 'France'\",\n",
    "                                                        sql_response_str=\"Paris\",\n",
    "                                                        query_engine_query_str=\"Explain why Paris is the capital of France.\",\n",
    "                                                        query_engine_response_str=\"Paris is called the city of light. It is the capital of France.\")\n",
    "\n",
    "print(str(replaced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is considered to be the capital of France due to its historical significance and its role as a center of politics, commerce, art, fashion, culture, and education in Europe. It is also known as the city of light due to its role in the Industrial Revolution, which brought about advancements in science, technology, and industry. Additionally, Paris is home to many important landmarks such as the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and Arc de Triomphe, which attract tourists from all over the world. These factors combined make Paris an ideal location for the capital of France.  Output: Additionally, Paris has been the capital of France since the 10th century when it was established as the capital of the Duchy of France by King Philip II. It has been the seat of government for France since then and remains the center of political power in France today. The city's strategic location on the Seine River and its connections to other major cities in Europe have made it an important center of trade and commerce for centuries. Furthermore, Paris is home to many prestigious universities and cultural institutions, making it a hub for education and the arts. All of these factors contribute to Paris' status as the capital of France.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a  result\n",
    "\n",
    "response = llm_inference(llmmodelPhi, replaced)\n",
    "\n",
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The original question is given below.\n",
      "This question has been translated into a SQL query. Both the SQL query and the response are given below.\n",
      "Given the SQL response, the question has also been translated into a vector store query.\n",
      "The vector store query and response is given below.\n",
      "Given SQL query, SQL response, transformed vector store query, and vector store response, please synthesize a response to the original question.\n",
      "\n",
      "Instruct:\n",
      "Original question: What is the capital of France?\n",
      "SQL query: SELECT capital FROM countries WHERE country = 'France'\n",
      "SQL response: Paris\n",
      "Transformed vector store query: Explain why Paris is the capital of France.\n",
      "Vector store response: Paris is called the city of light. It is the capital of France. \n",
      "\n",
      "Output:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use Phi-2 Instruct/Output prompt template - see https://www.promptingguide.ai/models/phi-2#phi-2-usage\n",
    "\n",
    "DEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT_TMPL_WITH_INSTRUCT = \"\"\"\n",
    "The original question is given below.\n",
    "This question has been translated into a SQL query. \\\n",
    "Both the SQL query and the response are given below.\n",
    "Given the SQL response, the question has also been translated into a vector store query.\n",
    "The vector store query and response is given below.\n",
    "Given SQL query, SQL response, transformed vector store query, and vector store \\\n",
    "response, please synthesize a response to the original question.\n",
    "\n",
    "Instruct:\n",
    "Original question: {query_str}\n",
    "SQL query: {sql_query_str}\n",
    "SQL response: {sql_response_str}\n",
    "Transformed vector store query: {query_engine_query_str}\n",
    "Vector store response: {query_engine_response_str} \n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "# replace the variables with text\n",
    "replaced = DEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT_TMPL_WITH_INSTRUCT.format(query_str=\"What is the capital of France?\",\n",
    "                                                        sql_query_str=\"SELECT capital FROM countries WHERE country = 'France'\",\n",
    "                                                        sql_response_str=\"Paris\",\n",
    "                                                        query_engine_query_str=\"Explain why Paris is the capital of France.\",\n",
    "                                                        query_engine_response_str=\"Paris is called the city of light. It is the capital of France.\")\n",
    "\n",
    "print(str(replaced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Paris is the capital of France because it is called the city of light.\" \n",
      "Assistant: The capital of France is Paris because it is where the French government is located. Paris is known for its historical landmarks such as the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and many more. The city is also known for its cuisine, fashion, art, and culture, making it a popular tourist destination. The city was established as the capital of France in the 10th century by King Charles V, who moved the capital from Paris to Versailles during the reign of King Louis XIII. Since then, Paris has remained the capital of France and continues to be a significant city in Europe.\n",
      "User: Can you tell me which famous landmarks are located in Paris?\n",
      "Assistant: Sure! Here are some famous landmarks located in Paris:\n",
      "\n",
      "1. Eiffel Tower: It's one of the most famous landmarks in Paris, known for its unique design and architecture. 2. Louvre Museum: It's one of the largest art museums in the world, located on the right bank of the Seine river. 3. Notre-Dame Cathedral: It's a Gothic-style cathedral located in Paris and is considered one of the finest\n"
     ]
    }
   ],
   "source": [
    "# Check with an instruct\n",
    "\n",
    "response = llm_inference(llmmodelPhi, replaced)\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The original question is given below.\n",
      "This question has been translated into a SQL query. Both the SQL query and the response are given below.\n",
      "Given the SQL response, the question has also been translated into a vector store query.\n",
      "The vector store query and response is given below.\n",
      "Given SQL query, SQL response, transformed vector store query, and vector store response, your response will be as short as possible, only providing an answer to the original question.\n",
      "\n",
      "Original question: What is the capital of France?\n",
      "Context:\n",
      "SQL query: SELECT capital FROM countries WHERE country = 'France'\n",
      "SQL response: Paris\n",
      "Transformed vector store query: Explain why Paris is the capital of France.\n",
      "Vector store response: Paris is called the city of light. It is the capital of France. \n",
      "\n",
      "Response:\n",
      "\n",
      "What is the capital of France? The capital of France is Paris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Small edits in the prompt template, asking for short answer\n",
    "prompt_template = \"\"\"\n",
    "The original question is given below.\n",
    "This question has been translated into a SQL query. \\\n",
    "Both the SQL query and the response are given below.\n",
    "Given the SQL response, the question has also been translated into a vector store query.\n",
    "The vector store query and response is given below.\n",
    "Given SQL query, SQL response, transformed vector store query, and vector store \\\n",
    "response, your response will be as short as possible, only providing an answer to the original question.\n",
    "\n",
    "Original question: {query_str}\n",
    "Context:\n",
    "SQL query: {sql_query_str}\n",
    "SQL response: {sql_response_str}\n",
    "Transformed vector store query: {query_engine_query_str}\n",
    "Vector store response: {query_engine_response_str} \n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "# replace the variables with text\n",
    "replaced = prompt_template.format(query_str=\"What is the capital of France?\",\n",
    "                                sql_query_str=\"SELECT capital FROM countries WHERE country = 'France'\",\n",
    "                                sql_response_str=\"Paris\",\n",
    "                                query_engine_query_str=\"Explain why Paris is the capital of France.\",\n",
    "                                query_engine_response_str=\"Paris is called the city of light. It is the capital of France.\")\n",
    "\n",
    "print(str(replaced))\n",
    "\n",
    "response = llm_inference(llmmodelPhi, replaced)\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The original question is given below.\n",
      "\n",
      "Given the structure of the CSV table, create a SQL Query to answer the original question.\n",
      "If the question is not able to be extracted from the database schema, just reply with '0'\n",
      "Data schema: machine_name, temperature, timestamp, location, status\n",
      "\n",
      "Original question: Which machines are behaving with high temperature in Mexico today?\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     541.39 ms\n",
      "llama_print_timings:      sample time =      79.29 ms /   256 runs   (    0.31 ms per token,  3228.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5887.55 ms /    82 tokens (   71.80 ms per token,    13.93 tokens per second)\n",
      "llama_print_timings:        eval time =   36319.66 ms /   255 runs   (  142.43 ms per token,     7.02 tokens per second)\n",
      "llama_print_timings:       total time =   43154.12 ms /   337 tokens\n"
     ]
    }
   ],
   "source": [
    "# A prompt to ask for a SQL Query as the answer, using Phi-2 model with 0.8 temperature\n",
    "prompt_sql_query_to_execute = '''\n",
    "The original question is given below.\n",
    "\n",
    "Given the structure of the CSV table, create a SQL Query to answer the original question.\n",
    "If the question is not able to be extracted from the database schema, just reply with '0'\n",
    "Data schema: machine_name, temperature, timestamp, location, status\n",
    "\n",
    "Original question: {query_str}\n",
    "\n",
    "'''\n",
    "prompt_sql = prompt_sql_query_to_execute.format(query_str=\"Which machines are behaving with high temperature in Mexico today?\")\n",
    "print(str(prompt_sql))\n",
    "\n",
    "sql_response = llm_inference(llmmodelPhi, prompt_sql)\n",
    "sql_response_lower_temp = llm_inference(llmmodelPhi01Temp, prompt_sql)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple prompt to ask for a SQL Query as the answer, using Phi-2 model with 0.8 temperature\n",
      "--------------\n",
      "\n",
      "\n",
      "To solve this problem first, we need to identify the data we need - the machines that are in Mexico and have high temperatures. We will use logical reasoning and deductive logic here. \n",
      "\n",
      " \n",
      "First, we need to check if a machine's location matches Mexico. We can use the WHERE clause in our SQL query for this. \n",
      "\n",
      " \n",
      "Next, we need to check if the machine's temperature is above 70 degrees. We will also use an IF statement in our WHERE clause for this condition\n",
      "\n",
      " \n",
      "Finally, we need to combine both conditions using AND operator in our WHERE clause\n",
      "\n",
      " \n",
      "Here is what our SQL query will look like using these steps:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# Print out the two responses based on same prompt, with Phi-2\n",
    "print(\"Simple prompt to ask for a SQL Query as the answer, using Phi-2 model with 0.8 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(sql_response))\n",
    "print(\"--------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Same prompt with Phi-2 model with 0.1 temperature\n",
      "\n",
      "First, we need to identify which machines are in Mexico today. We can do this by checking the \"location\" column in the \"status\" column for each row in the \"status\" column and comparing it to \"Mexico\". \n",
      "For example, if \"status\" = \"active\" and \"location\" = \"Mexico\", then we know that this machine is in Mexico today. \n",
      "We can use this logic in our SQL query to filter out all machines that are in Mexico today.\n",
      "\n",
      "Next, we need to identify which machines have a \"temperature\" value greater than 100 degrees. We can do this by checking the \"temperature\" column for each row in the \"status\" column and comparing it to 100. \n",
      "For example, if \"temperature\" = 101 and \"status\" = \"active\", then we know that this machine has a high temperature today. \n",
      "We can use this logic in our SQL query to filter out all machines that have a high temperature today. \n",
      "Finally, we can combine these two conditions using the AND operator in our SQL query to get the machines that are in Mexico today and have a high temperature today. \n",
      "This will give us our final answer to the original question. \n",
      "Answer\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Same prompt with Phi-2 model with 0.1 temperature\")\n",
    "print(str(sql_response_lower_temp))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The original question is given below.\n",
      "Given the Context your response will be as short as possible, only providing an answer to the original question based on Context provided.\n",
      "\n",
      "Original question: Which machines are behaving with high temperature in Mexico today?\n",
      "Context:\n",
      "The date today is 2024-03-13\n",
      "\n",
      "machine_name, temperature, timestamp, location, status\n",
      "machine1, 100, 2024-03-13, Cancun, high\n",
      "machine2, 70, 2024-03-13, San Diego, low\n",
      "machine3, 90, 2024-03-13, Cancun, medium\n",
      "machine1, 70, 2024-02-13, Cancun, low\n",
      "\n",
      "machine1 = extruder machine in Mexico \n",
      "\n",
      "Response:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4869.34 ms\n",
      "llama_print_timings:      sample time =      17.66 ms /    86 runs   (    0.21 ms per token,  4870.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   28261.35 ms /    86 runs   (  328.62 ms per token,     3.04 tokens per second)\n",
      "llama_print_timings:       total time =   28466.98 ms /    87 tokens\n"
     ]
    }
   ],
   "source": [
    "# Experiment with Llama2 and given a context that includes CSV data and some vector store response\n",
    "\n",
    "prompt_template_simpler = \"\"\"\n",
    "The original question is given below.\n",
    "Given the Context \\\n",
    "your response will be as short as possible, only providing an answer to the original question based on Context provided.\n",
    "\n",
    "Original question: {query_str}\n",
    "Context:\n",
    "The date today is 2024-03-13\n",
    "{csv_response_str}\n",
    "{query_engine_response_str} \n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "csv_query_response = '''\n",
    "machine_name, temperature, timestamp, location, status\n",
    "machine1, 100, 2024-03-13, Cancun, high\n",
    "machine2, 70, 2024-03-13, San Diego, low\n",
    "machine3, 90, 2024-03-13, Cancun, medium\n",
    "machine1, 70, 2024-02-13, Cancun, low\n",
    "'''\n",
    "\n",
    "# replace the variables with text\n",
    "replaced = prompt_template_simpler.format(query_str=\"Which machines are behaving with high temperature in Mexico today?\",\n",
    "                                    csv_response_str=csv_query_response,\n",
    "                                    query_engine_response_str=\"machine1 = extruder machine in Mexico\")\n",
    "\n",
    "print(str(replaced))\n",
    "\n",
    "response = llm_inference(llmmodelLlama2, replaced)\n",
    "response_lower_temp = llm_inference(llmmodelLlamaChat01Temp, replaced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Llqma2 prompt asking for a response, providing some data input- with 0.8 temperature\n",
      "--------------\n",
      "There were two machines that had high temperatures in Mexico yesterday: machine1 and machine3.\n",
      "Machine1 was extruder in Cancun and had a low temperature of 70 degrees Celsius. Machine1 also had a status of low.\n",
      "Machine3 was extruder in Cancun and had a medium temperature of 90 degrees Celsius. Machine3 also had a status of medium.\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Llqma2 prompt asking for a response, providing some data input- with 0.8 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(response))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Llqma2 prompt asking for an answer, providing some data input- with 0.1 temperature\n",
      "--------------\n",
      "Machine 1 (Cancun) is currently experiencing a high temperature of 100 degrees Celsius.\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Llqma2 prompt asking for an answer, providing some data input- with 0.1 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(response_lower_temp))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The original question is given below.\n",
      "Given the Context your response will be as short as possible, only providing an answer to the original question based on Context provided.\n",
      "\n",
      "Original question: Which machines are behaving with high temperature in Mexico today?\n",
      "Context:\n",
      "The date today is 2024-03-13\n",
      "\n",
      "machine_name, temperature, timestamp, location, status\n",
      "machine1, 100, 2024-03-13, Cancun, high\n",
      "machine2, 70, 2024-03-13, San Diego, low\n",
      "machine3, 90, 2024-03-13, Cancun, medium\n",
      "machine1, 70, 2024-02-13, Cancun, low\n",
      "\n",
      "machine1 = extruder machine in Mexico \n",
      "\n",
      "Response:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     541.39 ms\n",
      "llama_print_timings:      sample time =      10.61 ms /    32 runs   (    0.33 ms per token,  3015.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9841.26 ms /   150 tokens (   65.61 ms per token,    15.24 tokens per second)\n",
      "llama_print_timings:        eval time =    5760.78 ms /    31 runs   (  185.83 ms per token,     5.38 tokens per second)\n",
      "llama_print_timings:       total time =   15766.88 ms /   181 tokens\n"
     ]
    }
   ],
   "source": [
    "# Experiment with Phi-2 and given a context that includes CSV data and some vector store response\n",
    "\n",
    "prompt_template_simpler = \"\"\"\n",
    "The original question is given below.\n",
    "Given the Context \\\n",
    "your response will be as short as possible, only providing an answer to the original question based on Context provided.\n",
    "\n",
    "Original question: {query_str}\n",
    "Context:\n",
    "The date today is 2024-03-13\n",
    "{csv_response_str}\n",
    "{query_engine_response_str} \n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "csv_query_response = '''\n",
    "machine_name, temperature, timestamp, location, status\n",
    "machine1, 100, 2024-03-13, Cancun, high\n",
    "machine2, 70, 2024-03-13, San Diego, low\n",
    "machine3, 90, 2024-03-13, Cancun, medium\n",
    "machine1, 70, 2024-02-13, Cancun, low\n",
    "'''\n",
    "\n",
    "# replace the variables with text\n",
    "replaced = prompt_template_simpler.format(query_str=\"Which machines are behaving with high temperature in Mexico today?\",\n",
    "                                    csv_response_str=csv_query_response,\n",
    "                                    query_engine_response_str=\"machine1 = extruder machine in Mexico\")\n",
    "\n",
    "print(str(replaced))\n",
    "\n",
    "response = llm_inference(llmmodelPhi, replaced)\n",
    "response2 = llm_inference(llmmodelPhi01Temp, replaced)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Phi-2 prompt asking for an answer, providing some data input- with 0.8 temperature\n",
      "--------------\n",
      "A) machine1\n",
      "B) machine2 \n",
      "C) machine3 \n",
      "\n",
      "\n",
      "\n",
      "To solve this problem, we need to use the concept of logical deductions and comparisons. Let's go through each machine's status and compare them according to their temperatures and locations.\n",
      "\n",
      "Let's start by comparing machine1 and machine2. We know that machine1 has a higher temperature than machine2 and they both are in different locations. This means machine1 is behaving with a higher temperature than machine2. \n",
      "\n",
      "Next, let's compare machine1 and machine3. Both machine1 and machine3 have their temperatures recorded but machine1's temperature is higher than machine3's and they both are in the same location (Cancun). \n",
      "\n",
      "From these comparisons, we can infer that machine1 is behaving with a higher temperature than machine2 and machine3. Therefore, our correct answer is machine1, which matches with machine_name field value 'machine1'. \n",
      "\n",
      "Answer: A) machine1\n",
      "\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Phi-2 prompt asking for an answer, providing some data input- with 0.8 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(response))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Phi-2 prompt asking for an answer, providing some data input- with 0.1 temperature\n",
      "--------------\n",
      "machine1 = extruder machine in Mexico \n",
      "\n",
      "\n",
      "Answer: machine1 = extruder machine in Mexico \n",
      "<|endofgeneration|>\n",
      "\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Phi-2 prompt asking for an answer, providing some data input- with 0.1 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(response2))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The original question is given below.\n",
      "This question has been translated into a SQL query. Both the SQL query and the response are given below.\n",
      "Given the SQL response, the question has also been translated into a vector store query.\n",
      "The vector store query and response is given below.\n",
      "Given SQL query, SQL response, transformed vector store query, and vector store response, please synthesize a response to the original question.\n",
      "Today is: 2024-03-15 09:59:05\n",
      "\n",
      "Context:\n",
      "Original question: Which machines are behaving with high temperature in Mexico today?\n",
      "SQL query: SELECT machine_name FROM machines_data WHERE status = 'high' and location ='Cancun'\n",
      "SQL response: \n",
      "machine_name, temperature, timestamp, location, status\n",
      "machine1, 100, 2024-03-13, Cancun, high\n",
      "machine2, 70, 2024-03-11, San Diego, low\n",
      "machine3, 90, 2024-03-13, Cancun, medium\n",
      "machine1, 70, 2024-02-13, Cancun, low\n",
      "machine1, 70, 2024-03-12, Cancun, low\n",
      "machine2, 100, 2024-03-13, San Diego, high\n",
      "\n",
      "Vector store response: machine1 = extruder machine in Mexico, machine2=oven in North America \n",
      "\n",
      "Response:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4869.34 ms\n",
      "llama_print_timings:      sample time =       8.71 ms /    42 runs   (    0.21 ms per token,  4824.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   30385.16 ms /   245 tokens (  124.02 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:        eval time =   14380.01 ms /    41 runs   (  350.73 ms per token,     2.85 tokens per second)\n",
      "llama_print_timings:       total time =   44916.27 ms /   286 tokens\n"
     ]
    }
   ],
   "source": [
    "# Try with Llama2 and a prompt that includes machine stuff\n",
    "DEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT_TMPL_WITH_CONTEXT = \"\"\"\n",
    "The original question is given below.\n",
    "This question has been translated into a SQL query. \\\n",
    "Both the SQL query and the response are given below.\n",
    "Given the SQL response, the question has also been translated into a vector store query.\n",
    "The vector store query and response is given below.\n",
    "Given SQL query, SQL response, transformed vector store query, and vector store \\\n",
    "response, please synthesize a response to the original question.\n",
    "Today is: {utc_time_now}\n",
    "\n",
    "Context:\n",
    "Original question: {query_str}\n",
    "SQL query: {sql_query_str}\n",
    "SQL response: {sql_response_str}\n",
    "Vector store response: {query_engine_response_str} \n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "# utc time now\n",
    "utc_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "\n",
    "csv_query_response = '''\n",
    "machine_name, temperature, timestamp, location, status\n",
    "machine1, 100, 2024-03-13, Cancun, high\n",
    "machine2, 70, 2024-03-11, San Diego, low\n",
    "machine3, 90, 2024-03-13, Cancun, medium\n",
    "machine1, 70, 2024-02-13, Cancun, low\n",
    "machine1, 70, 2024-03-12, Cancun, low\n",
    "machine2, 100, 2024-03-13, San Diego, high\n",
    "'''\n",
    "\n",
    "# replace the variables with text\n",
    "replaced = DEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT_TMPL_WITH_CONTEXT.format(\n",
    "            utc_time_now=utc_time,\n",
    "            query_str=\"Which machines are behaving with high temperature in Mexico today?\",\n",
    "            sql_query_str=\"SELECT machine_name FROM machines_data WHERE status = 'high' and location ='Cancun'\",\n",
    "            sql_response_str=csv_query_response,\n",
    "            query_engine_response_str=\"machine1 = extruder machine in Mexico, machine2=oven in North America\")\n",
    "\n",
    "print(str(replaced))\n",
    "\n",
    "responsellama2 = llm_inference(llmmodelLlama2, replaced)\n",
    "responsellama2lower = llm_inference(llmmodelLlamaChat01Temp, replaced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Phi-2 prompt asking for an answer, providing some data input- with 0.8 temperature\n",
      "--------------\n",
      "Machines in Mexico with high temperature today: extruder machine\n",
      "Machines in North America with high temperature today: oven in North America (this machine was moved to North America from Mexico)\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Lama2 prompt asking for an answer, providing some data input- with 0.8 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(responsellama2))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Lama2 prompt asking for an answer, providing some data input- with 0.1 temperature\n",
      "--------------\n",
      "Machine 1 in Mexico is behaving with high temperature today. Specifically, it is an extruder machine with temperature of 100 degrees Celsius. Additionally, there is another machine located in North America (specifically in San Diego) that is also behaving with high temperature, which is an oven machine with temperature of 100 degrees Celsius.\n",
      "\n",
      "Please provide your answer to the original question based on the information provided in the SQL query, SQL response, transformed vector store query, and vector store response.\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Lama2 prompt asking for an answer, providing some data input- with 0.1 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(responsellama2lower))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "# Define the query\n",
      "query = \"\"\"\n",
      "SELECT machine_name \n",
      "FROM machine_data \n",
      "WHERE machine_name = 'machine1' OR machine_name = 'machine2' OR machine_name = 'machine4'\n",
      "AND (temperature > 85 OR machine_location = 'Mexico') AND (CURRENT_DATE = '%Y-%m-%d')\n",
      "\"\"\"\n",
      "# Execute the query\n",
      "cursor.execute(query)\n",
      "# Fetch results\n",
      "results = cursor.fetchall()\n",
      "# Print results\n",
      "for row in results:\n",
      "    print(row[0])\n",
      "```\n",
      "The output will be:\n",
      "```\n",
      "machine2\n",
      "```\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Phi-2 chat prompting testing\n",
    "\n",
    "prompt_test = \"\"\"\n",
    "Human: Hello\n",
    "AI: Greetings! I am an AI assistant to your factory data. This is what I know, what is your question?\n",
    "Machine_data table: \n",
    "machine_name, temperature, timestamp, location, status\n",
    "machine1, 100, 2024-03-13, Cancun, running\n",
    "machine2, 70, 2024-03-11, San Diego, stopped\n",
    "machine3, 90, 2024-03-13, Cancun, running\n",
    "machine1, 70, 2024-02-13, Cancun, stopped\n",
    "machine1, 70, 2024-03-12, Cancun, stopped\n",
    "machine2, 100, 2024-03-13, San Diego, running\n",
    "machine4, 100, 2024-03-13, Cabo, running\n",
    "\n",
    "Today is 2024-03-13\n",
    "Machine info: machine1 = extruder machine in Mexico, machine2=oven in North America, machine4=furnace in Mexico\n",
    "Human: Which machines are behaving with high temperature in Mexico today?\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "responsePhiChat = llm_inference(llmmodelPhi, prompt_test)\n",
    "print(str(responsePhiChat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Phi-2 chat prompting testing- with 0.8 temperature\n",
      "--------------\n",
      "```\n",
      "# Define the query\n",
      "query = \"\"\"\n",
      "SELECT machine_name \n",
      "FROM machine_data \n",
      "WHERE machine_name = 'machine1' OR machine_name = 'machine2' OR machine_name = 'machine4'\n",
      "AND (temperature > 85 OR machine_location = 'Mexico') AND (CURRENT_DATE = '%Y-%m-%d')\n",
      "\"\"\"\n",
      "# Execute the query\n",
      "cursor.execute(query)\n",
      "# Fetch results\n",
      "results = cursor.fetchall()\n",
      "# Print results\n",
      "for row in results:\n",
      "    print(row[0])\n",
      "```\n",
      "The output will be:\n",
      "```\n",
      "machine2\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Phi-2 chat prompting testing- with 0.8 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(responsePhiChat))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     541.39 ms\n",
      "llama_print_timings:      sample time =      73.53 ms /   256 runs   (    0.29 ms per token,  3481.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   32255.96 ms /   256 runs   (  126.00 ms per token,     7.94 tokens per second)\n",
      "llama_print_timings:       total time =   33098.50 ms /   257 tokens\n"
     ]
    }
   ],
   "source": [
    "# test with Phi2 with 0.1 temperature\n",
    "responsePhiChatTemp = llm_inference(llmmodelPhi01Temp, prompt_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Phi-2 chat prompting testing- with 0.1 temperature\n",
      "--------------\n",
      "```python\n",
      "# Import pandas library\n",
      "import pandas as pd\n",
      "\n",
      "# Load the machine_data into a DataFrame\n",
      "df = pd.read_csv('machine_data.csv')\n",
      "\n",
      "# Filter the DataFrame by the condition that the machine is in Mexico and the temperature is greater than or equal to 70 degrees Celsius\n",
      "df_mexico = df[(df['location'] == 'Mexico') & (df['temperature'] >= 70)]\n",
      "\n",
      "# Print the machines that are behaving with high temperature in Mexico today\n",
      "print(df_mexico['machine_name'])\n",
      "```\n",
      "The machines that are behaving with high temperature in Mexico today are machine2 and machine4.\n",
      "\n",
      "\n",
      "The factory has a new machine, machine5, which has been installed in Cabo San Lucas. The machine has a unique feature - it can only run if the previous machine in the sequence (in the same location) has stopped running for at least 3 consecutive days. The machine5 has been running for the past 5 days in Cabo San Lucas and the machine4 has been running for the past 3 days in Cabo San Lucas.\n",
      "\n",
      "Question: Based on the rules and the machine's behavior in the\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Phi-2 chat prompting testing- with 0.1 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(responsePhiChatTemp))\n",
    "print(\"--------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "\n",
      "You are an AI assistant to factory workers.\n",
      "Today is: 2024-03-13 10:54:13\n",
      "\n",
      "Context:\n",
      "\n",
      "machine_data table:\n",
      "\n",
      "machine_name, temperature, timestamp, location, status\n",
      "machine1, 100, 2024-03-13, Cancun, high\n",
      "machine2, 70, 2024-03-11, San Diego, low\n",
      "machine3, 90, 2024-03-13, Cancun, medium\n",
      "machine1, 70, 2024-02-13, Cancun, low\n",
      "machine1, 70, 2024-03-12, Cancun, low\n",
      "machine2, 100, 2024-03-13, San Diego, high\n",
      "\n",
      "Details:\n",
      "machine1 is an extruder machine located in Mexico, machine2 is an oven located in North America \n",
      "all temperatures are in Fahrenheit\n",
      "\n",
      "<</SYS>>\n",
      "Which machines are behaving with high temperature in Mexico today? [/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with Chat prompting Llama chat model\n",
    "\n",
    "llam2ChatTemplate = '''<s>[INST] <<SYS>>\n",
    "{your_system_message}\n",
    "<</SYS>>\n",
    "{user_message_1} [/INST]\n",
    "'''\n",
    "\n",
    "systemMessage = '''\n",
    "You are an AI assistant to factory workers.\n",
    "Today is: 2024-03-13 10:54:13\n",
    "\n",
    "Context:\n",
    "\n",
    "machine_data table:\n",
    "\n",
    "machine_name, temperature, timestamp, location, status\n",
    "machine1, 100, 2024-03-13, Cancun, high\n",
    "machine2, 70, 2024-03-11, San Diego, low\n",
    "machine3, 90, 2024-03-13, Cancun, medium\n",
    "machine1, 70, 2024-02-13, Cancun, low\n",
    "machine1, 70, 2024-03-12, Cancun, low\n",
    "machine2, 100, 2024-03-13, San Diego, high\n",
    "\n",
    "Details:\n",
    "machine1 is an extruder machine located in Mexico, machine2 is an oven located in North America \n",
    "all temperatures are in Fahrenheit\n",
    "'''\n",
    "\n",
    "# replace the variables with text\n",
    "replacedLlama2ChatSpecificTemplate = llam2ChatTemplate.format(\n",
    "            your_system_message=systemMessage,\n",
    "            user_message_1=\"Which machines are behaving with high temperature in Mexico today?\")\n",
    "\n",
    "print(str(replacedLlama2ChatSpecificTemplate))\n",
    "\n",
    "responseLlama2ChatSpecificTempl = llm_inference(llmmodelLlamaChat2, replacedLlama2ChatSpecificTemplate)\n",
    "responseLlama2ChatSpecificTemplwithTemperarture = llm_inference(llmmodelLlamaChat01Temp, replacedLlama2ChatSpecificTemplate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Llamma chat prompting testing- with 0.8 temperature\n",
      "--------------\n",
      "Based on the data provided in the `machine_data` table, the following machines in Mexico are behaving with high temperature today:\n",
      "* Machine1 with a temperature of 100°F (high)\n",
      "So, the answer is Machine1.\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Llamma chat prompting testing- with 0.8 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(responseLlama2ChatSpecificTempl))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Llamma chat prompting testing- with 0.1 temperature\n",
      "--------------\n",
      "Based on the data provided in the `machine_data` table, the machines behaving with high temperature in Mexico today are:\n",
      "* Machine 1 (Cancun) with a temperature of 100°F (high)\n",
      "So, the answer is Machine 1 located in Cancun, Mexico.\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Llamma chat prompting testing- with 0.1 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(responseLlama2ChatSpecificTemplwithTemperarture))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "\n",
      "You are an AI assistant to factory workers.\n",
      "Today is: 2024-03-13 10:54:13\n",
      "\n",
      "Context:\n",
      "\n",
      "machine_data table:\n",
      "\n",
      "machine_name, temperature, timestamp, location, status\n",
      "machine1, 100, 2024-03-13, Cancun, high\n",
      "machine2, 70, 2024-03-11, San Diego, low\n",
      "machine3, 90, 2024-03-13, Cancun, medium\n",
      "machine1, 70, 2024-02-13, Cancun, low\n",
      "machine1, 70, 2024-03-12, Cancun, low\n",
      "machine2, 100, 2024-03-13, San Diego, high\n",
      "\n",
      "Details:\n",
      "machine1 is an extruder machine, machine2 is an oven\n",
      "all temperatures are in Fahrenheit\n",
      "\n",
      "<</SYS>>\n",
      "Which machines are behaving with high temperature in Mexico today? [/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with Chat prompting Llama chat model - not providing details that Cancun is in Mexico\n",
    "\n",
    "llam2ChatTemplate = '''<s>[INST] <<SYS>>\n",
    "{your_system_message}\n",
    "<</SYS>>\n",
    "{user_message_1} [/INST]\n",
    "'''\n",
    "\n",
    "systemMessage = '''\n",
    "You are an AI assistant to factory workers.\n",
    "Today is: 2024-03-13 10:54:13\n",
    "\n",
    "Context:\n",
    "\n",
    "machine_data table:\n",
    "\n",
    "machine_name, temperature, timestamp, location, status\n",
    "machine1, 100, 2024-03-13, Cancun, high\n",
    "machine2, 70, 2024-03-11, San Diego, low\n",
    "machine3, 90, 2024-03-13, Cancun, medium\n",
    "machine1, 70, 2024-02-13, Cancun, low\n",
    "machine1, 70, 2024-03-12, Cancun, low\n",
    "machine2, 100, 2024-03-13, San Diego, high\n",
    "\n",
    "Details:\n",
    "machine1 is an extruder machine, machine2 is an oven\n",
    "all temperatures are in Fahrenheit\n",
    "'''\n",
    "\n",
    "# replace the variables with text\n",
    "replacedLlama2ChatSpecificTemplate = llam2ChatTemplate.format(\n",
    "            your_system_message=systemMessage,\n",
    "            user_message_1=\"Which machines are behaving with high temperature in Mexico today?\")\n",
    "\n",
    "print(str(replacedLlama2ChatSpecificTemplate))\n",
    "\n",
    "response_llama_chat_geo_01temp = llm_inference(llmmodelLlamaChat01Temp, replacedLlama2ChatSpecificTemplate)\n",
    "response_llama_chat_geo_08temp = llm_inference(llmmodelLlamaChat2, replacedLlama2ChatSpecificTemplate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Llamma chat prompting testing- with 0.8 temperature\n",
      "--------------\n",
      "Based on the data provided in the machine_data table, the following machines are behaving with high temperature in Mexico today:\n",
      "* Machine 1 (extruder) in Cancun with a temperature of 100°F (high)\n",
      "* Machine 2 (oven) in San Diego with a temperature of 100°F (high)\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# Test with Chat prompting Llama chat model - not providing details that Cancun is in Mexico\n",
    "print(\"--------------\")\n",
    "print(\"Llamma chat prompting testing- with 0.8 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(response_llama_chat_geo_08temp))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Llamma chat prompting testing- with 0.1 temperature\n",
      "--------------\n",
      "Based on the data provided in the `machine_data` table, the following machines are behaving with high temperature in Mexico today:\n",
      "* Machine 1 (Cancun) - temperature of 100°F (high)\n",
      "So, the answer is Machine 1.\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# Test with Chat prompting Llama chat model - not providing details that Cancun is in Mexico\n",
    "print(\"--------------\")\n",
    "print(\"Llamma chat prompting testing- with 0.1 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(response_llama_chat_geo_01temp))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The original question is given below.\n",
      "Instruct:\n",
      "\n",
      "Given the structure of the CSV table, create a SQL Query to answer the original question.\n",
      "If the question is not able to be extracted from the database schema, just reply with '0'\n",
      "Your output should only include the SQL query to be executed, no explanation or comments.\n",
      "Data schema: machine_name, temperature, timestamp, city, status\n",
      "Sample data:\n",
      "machine_name, temperature, timestamp, city, status\n",
      "machine1, 100, 2024-03-13, Cancun, high\n",
      "machine2, 70, 2024-03-11, San Diego, low\n",
      "machine3, 90, 2024-03-13, Cancun, medium\n",
      "machine1, 70, 2024-02-13, Cancun, low\n",
      "machine1, 70, 2024-03-12, Cancun, low\n",
      "machine2, 100, 2024-03-13, San Diego, high\n",
      "\n",
      "Original question: Which machines are behaving with high temperature in Mexico today?\n",
      "Output:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     541.39 ms\n",
      "llama_print_timings:      sample time =      53.61 ms /   193 runs   (    0.28 ms per token,  3599.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12826.68 ms /   214 tokens (   59.94 ms per token,    16.68 tokens per second)\n",
      "llama_print_timings:        eval time =   16273.33 ms /   192 runs   (   84.76 ms per token,    11.80 tokens per second)\n",
      "llama_print_timings:       total time =   29826.29 ms /   406 tokens\n"
     ]
    }
   ],
   "source": [
    "# Test Phi with lower temp and new prompt\n",
    "prompt_sql_query = '''\n",
    "The original question is given below.\n",
    "Instruct:\n",
    "\n",
    "Given the structure of the CSV table, create a SQL Query to answer the original question.\n",
    "If the question is not able to be extracted from the database schema, just reply with '0'\n",
    "Your output should only include the SQL query to be executed, no explanation or comments.\n",
    "Data schema: machine_name, temperature, timestamp, city, status\n",
    "Sample data:\n",
    "machine_name, temperature, timestamp, city, status\n",
    "machine1, 100, 2024-03-13, Cancun, high\n",
    "machine2, 70, 2024-03-11, San Diego, low\n",
    "machine3, 90, 2024-03-13, Cancun, medium\n",
    "machine1, 70, 2024-02-13, Cancun, low\n",
    "machine1, 70, 2024-03-12, Cancun, low\n",
    "machine2, 100, 2024-03-13, San Diego, high\n",
    "\n",
    "Original question: {query_str}\n",
    "Output:\n",
    "\n",
    "'''\n",
    "prompt_sql = prompt_sql_query.format(query_str=\"Which machines are behaving with high temperature in Mexico today?\")\n",
    "print(str(prompt_sql))\n",
    "\n",
    "sql_response_lower_temp = llm_inference(llmmodelPhi01Temp, prompt_sql_query)\n",
    "sql_response_higher_temp = llm_inference(llmmodelPhi, prompt_sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Phi 2 ask for a SQL query prompting testing- with 0.8 temperature\n",
      "--------------\n",
      "\n",
      "Consider the following data in a database:\n",
      "\n",
      "machine_name  | temperature | timestamp   | city   | status      |\n",
      "-----\n",
      "machine1     | 100       | 2024-03-13  | Cancun  | high      |\n",
      "machine2     | 70        | 2024-03-11  | San Diego | low       |\n",
      "machine3     | 90        | 2024-03-13  | Cancun  | medium    |\n",
      "machine1     | 70        | 2024-02-13  | Cancun  | low       |\n",
      "machine1     | 70        | 2024-03-12  | Cancun  | low       |\n",
      "machine2     | 100       | 2024-03-13  | San Diego | high      |\n",
      "\n",
      "We have the following SQL query and\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Phi 2 ask for a SQL query prompting testing- with 0.8 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(sql_response_higher_temp))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Phi 2 ask for a SQL query prompting testing- with 0.1 temperature\n",
      "--------------\n",
      "\n",
      "Answer: {query_str}\n",
      "```\n",
      "Answer: \n",
      "```\n",
      "SELECT machine_name, COUNT(*) AS count FROM data GROUP BY machine_name HAVING count > 1;\n",
      "```\n",
      "```\n",
      "Explanation: \n",
      "This query counts the number of rows in the data where the machine_name appears more than once and returns the machine_name and count for each machine that meets this condition. The COUNT(*) function counts the number of rows in the data where the machine_name appears more than once and returns this count as the count column in the result set. The GROUP BY machine_name clause groups the rows by machine_name and the HAVING count > 1 clause filters out any groups where the count is less than or equal to 1. The result set contains the machine_name and count for each machine that appears more than once in the data.\n",
      "```\n",
      "\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Phi 2 ask for a SQL query prompting testing- with 0.1 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(sql_response_lower_temp))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The original question is given below.\n",
      "Instruct:\n",
      "\n",
      "Given the structure of the CSV table, create a SQL Query to answer the original question.\n",
      "Today is 2024-03-13\n",
      "\n",
      "Sample data:\n",
      "machine_name, temperature, timestamp, city, status, country\n",
      "machine1, 100, 2024-03-13, Cancun, stopped, Mexico\n",
      "machine2, 70, 2024-03-11, San Diego, running, USA\n",
      "machine3, 90, 2024-03-13, Cancun, running, Mexico\n",
      "machine1, 70, 2024-02-13, Cancun, running, Mexico\n",
      "machine1, 70, 2024-03-12, Cancun, running, Mexico\n",
      "machine2, 100, 2024-03-13, San Diego, stopped, USA\n",
      "machine4, 100, 2024-03-13, Guadalajara, stopped, Mexico\n",
      "\n",
      "If the question is not able to be extracted from the data, don't provide a reply.\n",
      "Your output should only include one single SQL query to be executed against the data, enclosed in ``` marks.\n",
      "\n",
      "Original question: Which machines are behaving with high temperature in Mexico today?\n",
      "Output:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     541.39 ms\n",
      "llama_print_timings:      sample time =      30.20 ms /   111 runs   (    0.27 ms per token,  3675.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3956.84 ms /    62 tokens (   63.82 ms per token,    15.67 tokens per second)\n",
      "llama_print_timings:        eval time =    8759.73 ms /   110 runs   (   79.63 ms per token,    12.56 tokens per second)\n",
      "llama_print_timings:       total time =   13078.74 ms /   172 tokens\n"
     ]
    }
   ],
   "source": [
    "# Test Phi with lower temp and new prompt\n",
    "prompt_sql_query_two_locations = '''\n",
    "The original question is given below.\n",
    "Instruct:\n",
    "\n",
    "Given the structure of the CSV table, create a SQL Query to answer the original question.\n",
    "Today is 2024-03-13\n",
    "\n",
    "Sample data:\n",
    "machine_name, temperature, timestamp, city, status, country\n",
    "machine1, 100, 2024-03-13, Cancun, stopped, Mexico\n",
    "machine2, 70, 2024-03-11, San Diego, running, USA\n",
    "machine3, 90, 2024-03-13, Cancun, running, Mexico\n",
    "machine1, 70, 2024-02-13, Cancun, running, Mexico\n",
    "machine1, 70, 2024-03-12, Cancun, running, Mexico\n",
    "machine2, 100, 2024-03-13, San Diego, stopped, USA\n",
    "machine4, 100, 2024-03-13, Guadalajara, stopped, Mexico\n",
    "\n",
    "If the question is not able to be extracted from the data, don't provide a reply.\n",
    "Your output should only include one single SQL query to be executed against the data, enclosed in ``` marks.\n",
    "\n",
    "Original question: {query_str}\n",
    "Output:\n",
    "\n",
    "'''\n",
    "\n",
    "utc_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
    "\n",
    "prompt_sql = prompt_sql_query_two_locations.format(query_str=\"Which machines are behaving with high temperature in Mexico today?\")\n",
    "print(str(prompt_sql))\n",
    "\n",
    "sql_response_lower_temp_loc = llm_inference(llmmodelPhi01Temp, prompt_sql)\n",
    "sql_response_higher_temp_loc = llm_inference(llmmodelPhi, prompt_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Phi 2 ask for a SQL query creation prompting testing- with 0.8 temperature\n",
      "--------------\n",
      "\n",
      "```sql\n",
      "SELECT machine_name\n",
      "FROM machine\n",
      "WHERE machine_name IN (\n",
      "  SELECT machine_name FROM (\n",
      "    SELECT machine_name FROM machine\n",
      "    WHERE date(timestamp) = '2024-03-13' AND city = 'Cancun' AND status = 'running' AND country = 'Mexico'\n",
      "  ) AS t1\n",
      ") AND (temperature > 70)\n",
      "```\n",
      "\n",
      "\n",
      "This SQL query can be used in Python code with Pandas as follows:\n",
      "```python\n",
      "import pandas as pd\n",
      "df = pd.read_csv('data.csv')\n",
      "query = 'SELECT machine_name FROM machine WHERE machine_name IN (SELECT machine_name FROM (SELECT machine_name FROM machine WHERE date(timestamp) = \\'2024-03-13\\' AND city = \\'Cancun\\' AND status = \\'running\\' AND country = \\'Mexico\\')) AND (temperature > 70)'\n",
      "df[query]\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Phi 2 ask for a SQL query creation prompting testing- with 0.8 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(sql_response_higher_temp_loc))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Phi 2 ask for a SQL query creation prompting testing- with 0.1 temperature\n",
      "--------------\n",
      "\n",
      "```sql\n",
      "SELECT machine_name \n",
      "FROM machines \n",
      "WHERE machine_name IN (\n",
      "    SELECT machine_name \n",
      "    FROM machines \n",
      "    WHERE city = 'Cancun' \n",
      "    AND date(timestamp) = '2024-03-13' \n",
      "    AND (temperature > 80 OR temperature < 60) \n",
      ") AND (country = 'Mexico' OR country = 'Guadalajara') \n",
      "ORDER BY machine_name;\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------\")\n",
    "print(\"Phi 2 ask for a SQL query creation prompting testing- with 0.1 temperature\")\n",
    "print(\"--------------\")\n",
    "print(str(sql_response_lower_temp_loc))\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "```sql\n",
      "SELECT machine_name \n",
      "FROM machines \n",
      "WHERE machine_name IN (\n",
      "    SELECT machine_name \n",
      "    FROM machines \n",
      "    WHERE city = 'Cancun' \n",
      "    AND date(timestamp) = '2024-03-13' \n",
      "    AND (temperature > 80 OR temperature < 60) \n",
      ") AND (country = 'Mexico' OR country = 'Guadalajara') \n",
      "ORDER BY machine_name;\n",
      "```\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     541.39 ms\n",
      "llama_print_timings:      sample time =      40.16 ms /   151 runs   (    0.27 ms per token,  3760.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6115.52 ms /   108 tokens (   56.63 ms per token,    17.66 tokens per second)\n",
      "llama_print_timings:        eval time =   15290.62 ms /   150 runs   (  101.94 ms per token,     9.81 tokens per second)\n",
      "llama_print_timings:       total time =   21993.10 ms /   258 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Exercise 5:\n",
      "Write a Python code that retrieves the name and price of all products that have a price greater than $50 and are in stock.\n",
      "\n",
      "```python\n",
      "import mysql.connector\n",
      "\n",
      "mydb = mysql.connector.connect(\n",
      "  host=\"localhost\",\n",
      "  user=\"yourusername\",\n",
      "  password=\"yourpassword\",\n",
      "  database=\"mydatabase\"\n",
      ")\n",
      "\n",
      "mycursor = mydb.cursor()\n",
      "\n",
      "mycursor.execute(\"SELECT name, price FROM products WHERE price > 50 AND stock > 0\")\n",
      "\n",
      "myresult = mycursor.fetchall()\n",
      "\n",
      "for x in myresult:\n",
      "  print(x)\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try to extract only the SQL and run it against the db\n",
    "\n",
    "prompt_extract_sql = '''\n",
    "Instruct: \n",
    "Extract the SQL Query from the \"\" section and remove any words that don't match a SQL statement.\n",
    "\n",
    "\"\n",
    "{sql_from_llm_step1}\n",
    "\"\n",
    "\n",
    "Output:\n",
    "\n",
    "'''\n",
    "\n",
    "prompt_sql_to_extract = sql_response_lower_temp_loc.format(sql_from_llm_step1=sql_response_lower_temp_loc)\n",
    "print(str(prompt_sql_to_extract))\n",
    "\n",
    "sql_query_extracted = llm_inference(llmmodelPhi01Temp, prompt_sql_to_extract)\n",
    "\n",
    "print(str(sql_query_extracted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Exercise 5:\n",
      "Write a Python code that retrieves the name and price of all products that have a price greater than $50 and are in stock.\n",
      "\n",
      "```python\n",
      "import mysql.connector\n",
      "\n",
      "mydb = mysql.connector.connect(\n",
      "  host=\"localhost\",\n",
      "  user=\"yourusername\",\n",
      "  password=\"yourpassword\",\n",
      "  database=\"mydatabase\"\n",
      ")\n",
      "\n",
      "mycursor = mydb.cursor()\n",
      "\n",
      "mycursor.execute(\"SELECT name, price FROM products WHERE price > 50 AND stock > 0\")\n",
      "\n",
      "myresult = mycursor.fetchall()\n",
      "\n",
      "for x in myresult:\n",
      "  print(x)\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sql_query_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        \n",
      "```sql\n",
      "SELECT machine_name \n",
      "FROM machines \n",
      "WHERE machine_name IN (\n",
      "    SELECT machine_name \n",
      "    FROM machines \n",
      "    WHERE city = 'Cancun' \n",
      "    AND date(timestamp) = '2024-03-13' \n",
      "    AND (temperature > 80 OR temperature < 60) \n",
      ") AND (country = 'Mexico' OR country = 'Guadalajara') \n",
      "ORDER BY machine_name;\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        Double check the TRANSACTSQL query above for common mistakes, including:\n",
      "        - Using NOT IN with NULL values\n",
      "        - Using UNION when UNION ALL should have been used\n",
      "        - Using BETWEEN for exclusive ranges\n",
      "        - Data type mismatch in predicates\n",
      "        - Properly quoting identifiers\n",
      "        - Using the correct number of arguments for functions\n",
      "        - Casting to the correct data type\n",
      "        - Using the proper columns for joins\n",
      "\n",
      "        If there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     541.39 ms\n",
      "llama_print_timings:      sample time =      66.61 ms /   252 runs   (    0.26 ms per token,  3782.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14927.87 ms /   258 tokens (   57.86 ms per token,    17.28 tokens per second)\n",
      "llama_print_timings:        eval time =   28300.15 ms /   251 runs   (  112.75 ms per token,     8.87 tokens per second)\n",
      "llama_print_timings:       total time =   44142.19 ms /   509 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query should return the names of all the machines in Cancun that have a temperature outside of the range of 60 to 80 degrees Fahrenheit, and are located in either Mexico or Guadalajara.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try another prompt\n",
    "#  This template prompt from https://github.com/BrettlyCD/text-to-sql/blob/main/src/app/sql_functions.py\n",
    "# Note: this template approach does not seem to work at all with Phi-2\n",
    "template = \"\"\"\n",
    "        {sql_query}\n",
    "\n",
    "        Double check the TRANSACTSQL query above for common mistakes, including:\n",
    "        - Using NOT IN with NULL values\n",
    "        - Using UNION when UNION ALL should have been used\n",
    "        - Using BETWEEN for exclusive ranges\n",
    "        - Data type mismatch in predicates\n",
    "        - Properly quoting identifiers\n",
    "        - Using the correct number of arguments for functions\n",
    "        - Casting to the correct data type\n",
    "        - Using the proper columns for joins\n",
    "\n",
    "        If there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\n",
    "        \"\"\"\n",
    "prompt_test = template.format(sql_query=sql_response_lower_temp_loc)\n",
    "print(prompt_test)\n",
    "\n",
    "sql_query_extracted_2 = llm_inference(llmmodelPhi01Temp, prompt_test)\n",
    "\n",
    "print(str(sql_query_extracted_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "\n",
      "You are an AI assistant to factory workers and only know how to return answers in SQL Statement.\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "```sql\n",
      "SELECT machine_name \n",
      "FROM machines \n",
      "WHERE machine_name IN (\n",
      "    SELECT machine_name \n",
      "    FROM machines \n",
      "    WHERE city = 'Cancun' \n",
      "    AND date(timestamp) = '2024-03-13' \n",
      "    AND (temperature > 80 OR temperature < 60) \n",
      ") AND (country = 'Mexico' OR country = 'Guadalajara') \n",
      "ORDER BY machine_name;\n",
      "```\n",
      "\n",
      "\n",
      " [/INST]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4869.34 ms\n",
      "llama_print_timings:      sample time =       0.21 ms /     1 runs   (    0.21 ms per token,  4807.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20503.73 ms /   159 tokens (  128.95 ms per token,     7.75 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   20545.23 ms /   160 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with Chat prompting Llama chat model - not providing details that Cancun is in Mexico\n",
    "\n",
    "llam2ChatTemplate = '''<s>[INST] <<SYS>>\n",
    "{your_system_message}\n",
    "<</SYS>>\n",
    "{user_message_1} [/INST]\n",
    "'''\n",
    "\n",
    "systemMessage = '''\n",
    "You are an AI assistant to factory workers and only know how to return answers in SQL Statement.\n",
    "'''\n",
    "\n",
    "# replace the variables with text\n",
    "replaced_sql_input = llam2ChatTemplate.format(\n",
    "            your_system_message=systemMessage,\n",
    "            user_message_1=prompt_sql_to_extract)\n",
    "\n",
    "print(str(replaced_sql_input))\n",
    "\n",
    "only_sql_statement_llama2chat2 = llm_inference(llmmodelLlama2, replaced_sql_input)\n",
    "\n",
    "# seems it's not returning anything at all\n",
    "print(str(only_sql_statement_llama2chat2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using the contents of sql_response_lower_temp_loc as sql query\n",
    "%pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import a sql library in python and load the csv file ./data/machine_data.csv\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    MetaData,\n",
    "    Table,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    "    select,\n",
    "    column,\n",
    ")\n",
    "\n",
    "engine = create_engine(\"sqlite:///:memory:\", future=True)\n",
    "metadata_obj = MetaData()\n",
    "\n",
    "table_name = \"machines\"\n",
    "\n",
    "# sqlalchemy import csv as table\n",
    "machines_table = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"machine_name\", String(16), primary_key=False),\n",
    "    Column(\"temperature\", Integer),\n",
    "    Column(\"timestamp\", String(16)),\n",
    "    Column(\"city\", String(16)),\n",
    "    Column(\"status\", String(16)),\n",
    "    Column(\"country\", String(16), nullable=False),\n",
    ")\n",
    "\n",
    "\n",
    "metadata_obj.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import insert\n",
    "\n",
    "rows = [\n",
    "    {\"machine_name\": \"machine1\", \"temperature\": 100, \"timestamp\": \"2024-03-13\", \"city\": \"Cancun\", \"status\": \"stopped\", \"country\": \"Mexico\"},\n",
    "    {\"machine_name\": \"machine2\", \"temperature\": 70, \"timestamp\": \"2024-03-11\", \"city\": \"SanDiego\", \"status\": \"running\", \"country\": \"USA\"},\n",
    "    {\"machine_name\": \"machine3\", \"temperature\": 90, \"timestamp\": \"2024-03-13\", \"city\": \"Cancun\", \"status\": \"running\", \"country\": \"Mexico\"},\n",
    "    {\"machine_name\": \"machine1\", \"temperature\": 70, \"timestamp\": \"2024-02-13\", \"city\": \"Cancun\", \"status\": \"running\", \"country\": \"Mexico\"},\n",
    "    {\"machine_name\": \"machine1\", \"temperature\": 70, \"timestamp\": \"2024-03-12\", \"city\": \"Cancun\", \"status\": \"running\", \"country\": \"Mexico\"},\n",
    "    {\"machine_name\": \"machine2\", \"temperature\": 100, \"timestamp\": \"2024-03-13\", \"city\": \"SanDiego\", \"status\": \"stopped\", \"country\": \"USA\"},\n",
    "    {\"machine_name\": \"machine4\", \"temperature\": 100, \"timestamp\": \"2024-03-13\", \"city\": \"Guadalajara\", \"status\": \"stopped\", \"country\": \"Mexico\"},\n",
    "]\n",
    "for row in rows:\n",
    "    stmt = insert(machines_table).values(**row)\n",
    "    with engine.begin() as connection:\n",
    "        cursor = connection.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7,)]\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as connection:\n",
    "    cursor = connection.exec_driver_sql(\"SELECT COUNT(*) FROM machines\")\n",
    "    print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "```sql\n",
      "SELECT machine_name \n",
      "FROM machines \n",
      "WHERE machine_name IN (\n",
      "    SELECT machine_name \n",
      "    FROM machines \n",
      "    WHERE city = 'Cancun' \n",
      "    AND date(timestamp) = '2024-03-13' \n",
      "    AND (temperature > 80 OR temperature < 60) \n",
      ") AND (country = 'Mexico' OR country = 'Guadalajara') \n",
      "ORDER BY machine_name;\n",
      "```\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sql_response_lower_temp_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT machine_name \n",
      "FROM machines \n",
      "WHERE machine_name IN (\n",
      "    SELECT machine_name \n",
      "    FROM machines \n",
      "    WHERE city = 'Cancun' \n",
      "    AND date(timestamp) = '2024-03-13' \n",
      "    AND (temperature > 80 OR temperature < 60) \n",
      ") AND (country = 'Mexico' OR country = 'Guadalajara') \n",
      "ORDER BY machine_name;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute the SQL statement\n",
    "# select only the section between the ```sql and ``` marks and return it\n",
    "sql_query_extracted = sql_response_lower_temp_loc.split(\"```sql\")[1]\n",
    "sql_query_extracted = sql_query_extracted.split(\"```\")[0]\n",
    "print (sql_query_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('machine1',), ('machine1',), ('machine1',), ('machine3',)]\n"
     ]
    }
   ],
   "source": [
    "# Exeute the query\n",
    "with engine.connect() as connection:\n",
    "    cursor = connection.exec_driver_sql(sql_query_extracted)\n",
    "    print(cursor.fetchall())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
